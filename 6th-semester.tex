\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T2A]{fontenc}

\usepackage[intlimits]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{algorithm2e}
\usepackage[backend=biber,style=authortitle]{biblatex}
\addbibresource{biblio-u.bib}
%\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\title{Устранение экспоненциальной сложности оценки стоимости бермудского опциона}
\author{Анастасия Миллер}
\date{СПбГУ, 6${}^{\mbox{\small ой}}$ семестр,~~ 322 гр. \\ \today} 

\setlength\parindent{0pt}
\setlength\parskip{0.5em}
\begin{document}
\maketitle
\section{Вступление}
\par В книге \cite{Glasserman2004} был предложен метод оценки американских опционов с конечным множеством дат погашения. Две оценки -- смещённая вверх и смещённая вниз -- получаются с помощью смоделированного дерева, которое вевтится при каждой возможности раннего погашения опциона. Оценки являются состоятельными (т.е. сходятся по вероятности к истинной цене опциона) и асимптотически несмещёнными.
\par Один из основных недостатков алгоритма -- его экспоненциальная сложность. Здесь же предлагается несколько подходов, которые заменят экспоненциальную сложность полиномиальной с одновременным увеличением <<случайности>> алгоритма.
\section{Общая идея алгоритма}
\par Начиная с некоторого момента $t_k$, когда общее число состояний достигнет некоторого $n$, мы перестанем генерировать дочерние вершины ко всем состояниям. В следующий момент времени, $t_{k+1}$, мы будем иметь всё так же $n$ состояний, а не $bn$. Этого можно достичь, если генерировать дочерние состояния не ко всем вершинам, а только к некоторым. К каким?
\subsection{Анализ распределения состояний с помощью гистограммы}
\par В том случае, когда состояние актива $S$ является числом в $\R ^1$, в качестве параметра $X$, распределение которого нас интересует, можно использовать  само $S$, иначе можно использовать $h(S)$. 
\par Деля интервал $\left[\min_{i\in 1:n} X_i ; \max_{i\in 1:n} X_i + \frac{1}{n}\right)$ на $k$ равных частей $\left[a_{k-1},a_k\right)$, где $a_0 = \min_{i\in 1:n} X_i$, $a_k = \max_{i\in 1:n} X_i$, мы можем определить частоты $f_k = \#\left\lbrace X_i \middle\vert X_i\in\left[a_{k-1},a_k\right)\right\rbrace / n$ попадания событий в различные части отрезка. Из состояний, сгруппированных на отрезке $\left[a_{k-1},a_k\right)$, мы также можем создать некоторый <<средний арифметический>> вектор, кооринаты которого будут являться средним арифметическим координат всех состояний, оказавшихся на данном отрезке, и уже для этого нового среднего состояния -- представителя отрезка -- генерировать дочерние вершины в количестве $n f_k$. Для всех состояний, оказавшихся в этом отрезке, дочерними вершинами будут являться все вершины, полученные от их представителя. 
\par Таким образом, количество рассматриваемых состояний не увеличится. С другой стороны, этот метод предполагает хранение в памяти всего дерева, а не только непосредственно обсчитываемой ветки, как это предполагалось в исходной работе \cite{Broadie1997}.
\subsection{Кластеризация состояний}
Для выделения родителей будущего поколения событий можно использовать не гистограммный подход, а кластеризацию существующего поколения. Так как состояния являются векторами в $\R^d$, в качестве метрики можно взять, например, <<улучшенную>> евклидову метрику в $\R^d$:
\[\mu\left(S_i,S_j\right) = \sqrt{\sum_{k=1}^d\frac{\left(s^i_k-s^j_k\right)}{c_k}} \text{,} \] 
где $c_k = \left\vert\max_{i\in 1:n} s^i_k - \min_{i\in 1:n} s^i_k\right\vert$, т.е. масштабирующий множитель, уравнивающий влияние различных компонент состояний на итоговое расстояние между ними.
\par Кластеризация может быть проведена по любому из известных алгоритмов, разберём один из наиболее простых и популярных алгоритмов кластеризации применительно к нашей задаче -- алгоритм $k$-средних.
\begin{algorithm}
	\SetAlgoLined
	\DontPrintSemicolon
	\SetKwFunction{Get}{Get}
	\SetKwFunction{Argmin}{argmin}
	\tcp{назначаем центрами кластеров случайно выбранные $S_k$}
	\For{$j \in 1:k$}{
		\Get{$\alpha$}\;
		$C_j = S_{\left\lceil n\alpha\right\rceil}, \forall i \in 1:j-1 C_i \not = C_j$\;
	}
	\For{$j \in 1:n$}{
		\tcp{центроидом для каждого состояния полагаем тот из центроидов, который ближе всего к данному состоянию}
		$S_j$.centroid = \Argmin{$i\in 1:k$, $\mu\left(S_j, C_i\right)$}\;
	}
	changed = true\;
	\Repeat{changed = true}{
		\For{$j \in 1:k$}{
			centroid = $\left\lbrace i\in 1: n\middle\vert S_i\text{.centroid} = C_j\right\rbrace$\;
			$C_j = \frac{1}{\#\left(\text{centroid}\right)}\sum\limits_{i \in \text{centroid}}S_i$\;
		}
		changed = false\;
		\For{$j \in 1:n$}{
			\tcp{пересчитываем принадлежность состояний центроидам}
			oldcentroid = $S_j$.centroid\;
			$S_j$.centroid = \Argmin{$i\in 1:k$, $\mu\left(S_j, C_i\right)$}\;
			changed = (oldcentroid == $S_j$.centroid)
		}
	}
\end{algorithm}
\par В этом случае набор центроидов $\left\lbrace C_j\right\rbrace_{j=1}^k$, можно использовать в качестве родителей для следующего поколения состояний.
\par Известные недостатки алгоритма $k$-средних, такие как сложность $O\left(2^n\right)$ в худшем случае, зависимость результатов от начального выбора центроидов (которая, впрочем, может быть частично устранена при модификации алгоритма, например, в версии $k$-means++, описанной в \cite{Arthur2007}) 
\nocite{*}
\printbibliography
\end{document}
%!TEX root = thesis.tex
\chapter{Сглаживание случайных деревьев для оценки стоимости Американского опциона} % (fold)
\label{cha:tree_pruning_for_american_option}

Метод случайных деревьев, рассмотренный в секции \ref{sec:classic_approaches:tree_estimator}, наряду с преимуществами в виде простоты реализации и точности (\textcolor{red}{сравнение точности классических методов за тики}), обладает одним существенным недостатком: из-за экспоненциальной сложности его фактически невозможно использовать для оценки <<настоящих>> Американских опционов, то есть при $m \to \infty$. В этой главе предлагается метод, позволяющий сохранить простоту и точность оценки и при этом использовать случайные деревья для сколь угодно часто исполняемых опционов.

В некотором смысле предлагаемый метод является комбинацией метода наименьших квадратов и метода случайных деревьев. Основная идея заключается в том, что, как только вершин дерева становится слишком много, меняется способ оценки величины $\E\left[V_i\left(X_i\right)|X_{i-1}=x\right]$: вместо рекурсивного построения дальнейшего дерева из точки $x$ используется регрессионная оценка условного математического ожидания (как в методе наименьших квадратов).

\section{Общий алгоритм случайного дерева с прореживанием} % (fold)
\label{sec:tree_pruning:general_algorithm}

Схема метода представлена на рис.\,\ref{fig:pruned_tree}. Параметрами алгоритма являются высота дерева $h$, число ветвей $b$ и количество точек для оценки линейной регрессии $n$. Предположим для упрощения рассуждений, что $(m-1) \bmod (h-1) = 0$ (с учётом того, что $m \to \infty$, это не умаляет общности рассуждений). В дальнейших рассуждениях $m$ моментов выполнения опциона $t_0, \dots, t_{m-1}$ равномерно распределены по временному отрезку ${[0; T]}$: $t_i = i T / (m-1), \: i\in 0\mathbin{:} {m-1}$. 

\begin{figure}[h!]
    \centering
    \includegraphics{pruned_tree.pdf}
    \caption{Схема состояний для алгоритма сглаживания}
    \label{fig:pruned_tree}
    \footnotesize
    Красным выделены те (выбранные случайным образом) состояния, из которых дерево продолжает расти дальше. Для всех остальных состояний на шаге сглаживания (когда ширина всей схемы достигает установленного предела) стоимость удержания опциона оценивается с помощью регрессии, подобранной по результатам оценки по дереву для <<красных>> состояний.
\end{figure}

Общий алгоритм можно сформулировать следующим образом:

\begin{enumerate}[noitemsep]
	\item Из состояния $X_0$ моделируется дерево высоты $h$ с $b$ ветвями, исходящими из каждой вершины. Обозначим вершины этого дерева как $\prescript{0}{}{X}_i^{j_1\dots j_i},\; i\in{1\mathbin{:}{h-1}},\,j_i\in{1\mathbin{:}b}$.
	\item Из 
	множества состояний $\left\{\prescript{0}{}{X}_{h-1}^{j_1\dots j_{h-1}}\right\}$, соответствующих моменту $t_{h-1}$, выбираются случайным образом $n$ состояний $\left\{X_{h-1}^k\right\}_{k=1}^n$, из которых будет продолжаться построение дерева. На рис.\,\ref{fig:pruned_tree} $h=3$, множество состояний на момент $t_{h-1}$ --- это все точки, находящиеся на линии $t_2$, выбранные $n=2$ из них обозначены красным цветом.
	
	\item\label{step:grow_tree} Для каждого из состояний $X_{h-1}^k$ моделируется дерево высоты $h$ с $b$ ветвями, исходящими из каждой вершины (корень дерева -- $X_{h-1}^k$, остальные вершины обозначим как $\prescript{k}{}{X}_{h-1+i}^{j_1\dots j_i},\; k\in{1\mathbin{:}n},\,i\in{1\mathbin{:}{h-1}},\,j_i\in{1\mathbin{:}b}$). Так появляется множество состояний актива на момент $t_{2(h-1)}$ (на рис.\,\ref{fig:pruned_tree} это все точки на линии $t_4$).
	
	\item\label{step:prune_tree} Из полученного множества состояний актива $\left\{\prescript{k}{}{X}_{2(h-1)}^{j_1\dots j_{h-1}}\right\}$ на момент $t_{2(h-1)}$ выбираем случайным образом $n$ состояний $\left\{X_{2(h-1)}^i\right\}_{i=1}^n$, из которых будет продолжаться построение дерева (на рис.\,\ref{fig:pruned_tree} выбранные $n=2$ обозначены красным цветом).
	
	\item Шаги \ref{step:grow_tree}--\ref{step:prune_tree} повторяются для последующих моментов времени до тех пор, пока на шаге \ref{step:grow_tree} листья дерева не оказываются множеством состояний актива на момент $t_{m-1}$ (на рис.\,\ref{fig:pruned_tree} это все состояния в строке $t_{m-1}$). Несложно заметить, что для этого требуется $L = (m-h) / (h-1)$ повторений, включая первое. Обозначим номера моментов, в которые совершается прореживание дерева (шаг \ref{step:prune_tree}), как $p_l = l(h-1),\;l\in{1\mathbin{:}L}$.

	\item\label{step:tree_estimation} Теперь по смоделированной системе можно построить оценку. Для последнего набора деревьев, корни которых находятся в моменте $t_{p_L} = t_{(m-1) - (h-1)} = t_{m-h}$, а листья -- в моменте $t_{m-1}$ (на рис\,\ref{fig:pruned_tree} это $n=2$ дерева, расположенных на линиях $t_{m-3}-t_{m-1}$) оценка строится в точности как в методе случайных деревьев (\ref{eq:upper},\,\ref{eq:lower})\footnote{главное --- не забыть, что $\deltat = t_{m-1} - t_{m-2} = T / (m-1)$ (в методе случайных деревьев высота дерева $h$ равна числу моментов исполнения опциона $m$, здесь же $m \neq h$)}:
	\begin{equation}\label{eq:pruned_tree_upper}\begin{aligned}
	    \prescript{k}{}{\Vhat}_{m-1}^{j_1 \ldots j_{h-1}} &= h_{m-1}\left(\prescript{k}{}{X}_{m-1}^{j_1 \ldots j_{h-1}}\right), \\
	    \prescript{k}{}{\Vhat}_{p_L+i}^{j_1 \ldots j_i} &= \max \left\lbrace h_{p_L+i} \left( \prescript{k}{}{X}_{p_L+i}^{j_1 \ldots j_i} \right), \frac{1}{b} \sum_{j = 1}^b \prescript{k}{}{\Vhat}_{p_L+i+1}^{j_1 \ldots j_i j}\right\rbrace,\;i\in{0\mathbin{:}{h-1}}.
	\end{aligned}\end{equation}
	В случае $i=0$ под вершиной $\prescript{k}{}{X}_{p_L+i}^{j_1 \ldots j_i}$ следует понимать $X_{p_L}^k$ (корень соответствующего дерева). Здесь используется только оценка сверху \eqref{eq:upper}, но разработать аналог для оценки снизу тоже вполне возможно.

	\item\label{step:estimate_continuation} Для всех $X_{p_L}^k$ (все красные состояния в строке $t_{p_L} = t_{m-h}=t_{m-3}$ на рис.\,\ref{fig:pruned_tree}) на шаге \ref{step:tree_estimation} получена оценка стоимости удержания опциона ($\frac{1}{b} \sum_{j = 1}^b \prescript{k}{}{\Vhat}_{p_L+1}^{j}$). По этим $n$ наблюдениям строится оценка для 
	% стоимости удержания опциона 
	$\E\left[V_{p_L+1}(X_{p_L+1}) \middle\vert X_{p_L} = x \right]$ с помощью линейной или непараметрической регрессии $\Vhat^{\mathrm{regr}.}_{p_L+1}(x)$. Положим теперь для всех состояний $\prescript{k}{}{X}_{p_L}^{j_1 \ldots j_{h-1}}$ на момент $t_{p_L}$ стоимость опциона равной 
	$$\prescript{k}{}{\Vhat}_{p_L}^{j_1 \ldots j_{h-1}} = \maxset{h_{p_L}\left(\prescript{k}{}{X}_{p_L}^{j_1 \ldots j_{h-1}}\right), \Vhat^{\mathrm{regr}.}_{p_L+1}\left(\prescript{k}{}{X}_{p_L}^{j_1 \ldots j_{h-1}}\right)}.$$
	То есть для всех тех вершин, из которых дерево не было продолжено, стоимость удержания опциона (для оценки которой и строится дальнейшее дерево) оценивается по регрессии, как в методе наименьших квадратов (секция \ref{sec:classic_approaches:least_squares}).

	\item\label{step:backward_tree} Теперь, когда для всех листьев из предыдущего поколения известна оценка $\prescript{k}{}{\Vhat}_{p_L}^{j_1 \ldots j_{h-1}}$, методом случайных деревьев можно получить оценки для стоимости опциона в точках $X_{p_{L-1}}^k$:
	$$\prescript{k}{}{\Vhat}_{p_{L-1}+i}^{j_1 \ldots j_i} = \max \left\lbrace h_{p_{L-1}+i} \left( \prescript{k}{}{X}_{p_{L-1}+i}^{j_1 \ldots j_i} \right), \frac{1}{b} \sum_{j = 1}^b \prescript{k}{}{\Vhat}_{p_{L-1}+i+1}^{j_1 \ldots j_i j}\right\rbrace, \: i\in{0\mathbin{:} {h-2}}.$$

	\item\label{step:repeat_backward} Шаги \ref{step:estimate_continuation}--\ref{step:backward_tree} повторяются для всех $p_l$ до тех пор, пока в конце шага \ref{step:backward_tree} алгоритм не окажется в вершине $X_0$. Оценка в $X_0$ и будет итоговой оценкой.
\end{enumerate}

Вычислительная сложность алгоритма --- $O(Lnb^h) = O(nb^h (m-h) / (h-1)) = O(mnb^h)$ по времени и столько же по памяти (так как в описанной выше реализации приходится хранить все промоделированные вершины). При $m\to\infty$ алгоритм обладает полиномиальной сложностью, в отличие от исходного метода случайных деревьев.

Предложенный алгоритм обладает одним существенным недостатком: наивная реализация требует хранить в памяти всё множество вершин от $X_0$ до $\prescript{k}{}{X}_{m-1}^{j_1 \ldots j_{h-1}}$. Это вполне осуществимо, но исходные алгоритмы (как метод случайных деревьев, так и метод наименьших квадратов) позволяют расходовать гораздо меньше памяти.

Простое решение этой проблемы состоит в том, чтобы вместо того, чтобы в моменты $t_{p_l}$ создавать новые деревья с корнями из уже существующих, выбирать точки в пространстве состояний случайным образом (так же, как это делается в методе стохастических сеток). Тогда вместо полного прохода вперёд (шаги 1--\ref{step:prune_tree}) и назад (шаги \ref{step:tree_estimation}--\ref{step:repeat_backward}) можно начинать генерировать деревья с последнего поколения (момент $p_L$) и идти только назад. В таком случае достаточно хранить одно поколение вершин, и сложность по памяти становится равной $O(nb^h)$, оставаясь постоянной при $m\to\infty$.

% section tree_pruning:general_algorithm (end)

% chapter tree_pruning_for_american_option (end)
\documentclass[12pt,twoside,titlepage,сa4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{csquotes}
\usepackage[T2A]{fontenc}

\usepackage[intlimits]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{algorithm2e}
\usepackage[backend=bibtex,style=numeric]{biblatex}
\addbibresource{biblio-u.bib}

\usepackage{statmodtitle}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{hyperref}
\newtheorem{theorem}{Теорема}
\newcommand{\ev}{\mathsf{E}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}

\renewcommand\topfraction{0.85}
\renewcommand\bottomfraction{0.85}
\renewcommand\textfraction{0.1}
\renewcommand\floatpagefraction{0.85}

\title{Устранение экспоненциальной сложности оценки стоимости бермудского опциона}
\author{Анастасия Миллер}
%\date{СПбГУ, 6${}^{\mbox{\small ой}}$ семестр,~~ 322 гр. \\ \today}
\facility{Кафедра статистического моделирования}
\supervisor{д.ф.-м.н.~Ермаков С.М.}

\setlength\parindent{0pt}
\setlength\parskip{0.5em}

\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Вступление}
	\par Метод оценки американских опционов с конечным числом дат погашения, основанный на моделировании дерева событий (метод случайных деревьев), был предложен в \citation{Glasserman2004} ещё в 2004 году. Этот метод моделирует изменение состояния базового актива через случайные деревья, разветвляющиеся в каждой из возможных дат раннего погашения опциона. При анализе деревьев могут быть получены две оценки: смещённая вверх и смещённая вниз, являющиеся асимптотически несмещёнными и дающие доверительный интервал для истинной цены опциона. 
	% \par В книге \cite{Glasserman2004} был предложен метод оценки американских опционов с конечным множеством дат погашения. Две оценки -- смещённая вверх и смещённая вниз -- получаются с помощью смоделированного дерева, которое вевтится при каждой возможности раннего погашения опциона. Оценки являются состоятельными (т.е. сходятся по вероятности к истинной цене опциона) и асимптотически несмещёнными.
	\par Один из основных недостатков алгоритма -- его экспоненциальная сложность. Здесь же предлагается несколько подходов, которые заменят экспоненциальную сложность полиномиальной с одновременным увеличением <<случайности>> алгоритма.
\section{Метод случайных деревьев}
	\paragraph{Обозначения и умолчания}
	\par Будем строить модель на примере американского опциона с конечным числом дат погашения $t_1, \ldots t_m$ (также называемого бермудским опционом). Мы также сузим класс решаемых нами задач до тех, в которых вся необходимая информация об активе, на который выписан рассматриваемый опцион, может быть представлена в виде Марковского процесса $X\left( t \right), t \in \left\lbrace t_i \right\rbrace_{i = 1}^m$ со значениями в $\mathbb{R}^d$. Для уменьшения объёма текста будем обозначать $X\left(t_i\right) \equiv X_i$. Положим также $h_i\left(x\right)$ -- размер выплаты по опциону в момент $t_i$ при том, что $x = X_i$ и опцион не был исполнен до этого, $V_i\left(x\right)$ -- стоимость опциона в момент $t_i$ при том, что $x = X_i$.
	\par Нетрудно видеть, что
		\begin{eqnarray}\label{eq:option-recursive}
			V_m\left(x\right) = h_m\left(x\right) \\
			V_{i-1}\left(x\right) = \max\left\lbrace h_{i-1}\left(x\right), \mathsf{E}\left[V_i\left(X_i\right)|X_{i-1}=x\right]\right\rbrace
		\end{eqnarray}
	 -- на каждом шаге мы выбираем наиболее выгодное решение. Здесь нас интересует значение $V_0\left(X_0\right)$.
	\subsection{Построение дерева и оценок}
	\par Вместо того, чтобы строить оценку, каким-либо образом стремящуюся к требуемому нами значению, мы построим две оценочные функции, оценивающие $V_n$ сверху и снизу. Пусть $\hat{V}_n\left(b\right)$ и $\hat{v}_n\left(b\right)$ -- такие оценки, зависящие от некоторого параметра $b$.
	\par Метод случайного дерева основан на моделировании цепи $X_0, X_1, \ldots X_n$. Зафиксируем параметр ветвления $b$. Из исходного состояния $X_0$ смоделируем $b$ независимых следующих состояний $X_1^1, X_1^2, \ldots X_1^b$, все с условием $X_1$. Для каждого $X_1^i$ снова смоделируем $b$ независимых последующих состояний $X_2^{i1}, \ldots X_2^{ib}$. На $m$-ом шаге будем иметь $b^m$ состояний, и это и есть источник основного недостатка этого метода -- его экспоненциальной алгоритмической сложности.
		\subsubsection{Оценка сверху}
		\par Определим $\hat{V}_i^{j_1, j_2 \ldots j_i}$, вдохновляясь \ref{eq:option-recursive}. В последних вершинах (листьях) дерева зададим
		\begin{equation}\label{eq:upper-terminal}
			\hat{V}_m^{j_1 \ldots j_m} = h_m\left(X_m^{j_1 \ldots j_m}\right)
		\end{equation}
		Идя вверх по дереву, зададим
		\begin{equation}\label{eq:upper-node}
			\hat{V}_i^{j_1 \ldots j_i} = \max \left\lbrace h_i \left( X_i^{j_1 \ldots j_i} \right), \frac{1}{b} \sum_{j = 1}^b \hat{V}_{i+1}^{j_1 \ldots j_i j}\right\rbrace 
		\end{equation}
		\par С помощью индукции можно доказать, что наша оценка уклоняется вверх в каждом узле
		\begin{theorem}
			$\forall i \in 1:n$
			\begin{equation*}
			\mathsf{E}\left[\hat{V}_i^{j_1\ldots j_i}|X_i^{j_1\ldots j_i}\right] \geq V_i\left(X_i^{j_1\ldots j_i}\right)
			\end{equation*}
		\end{theorem}
		\begin{proof}
			\par В листьях дерева неравенство выполняется как равенство по определению.
			\par Докажем, что если утверждение теоремы выполняется на $i+1$ шаге, то оно выполняется и на $i$. По определению
			\begin{equation*}
			\ev\left[\hat{V}_i^{j_1\ldots j_i}|X_i^{j_1\ldots j_i}\right] = \ev\left[ \max\left\lbrace h_i\left(X_i^{j_1\cdots j_i}\right), \frac{1}{b}\sum_{j = 1}^b \hat{V}_{i+1}^{j_1 \ldots j_i j}\right\rbrace | X_i^{j_1\cdots j_i} \right]
			\end{equation*}
			с помощью неравенства Йенсена ($\varphi\left(\ev\left[X\right]\right) \leqslant \ev\left[\varphi(X)\right]$) это можно оценить
			\begin{equation*}
			\ev\left[ \max\left\lbrace h_i\left(X_i^{j_1\cdots j_i}\right), \frac{1}{b}\sum_{j = 1}^b \hat{V}_{i+1}^{j_1 \ldots j_i j}\right\rbrace | X_i^{j_1\cdots j_i} \right] \geq \max\left\lbrace h_i\left(X_i^{j_1\cdots j_i}\right), \ev\left[ \frac{1}{b}\sum_{j = 1}^b \hat{V}_{i+1}^{j_1 \cdots j_i j} | X_i^{j_1\cdots j_i} \right] \right\rbrace
			\end{equation*}
			в силу того, что $\forall \, j \in 1:b \quad X_{i+1}^{j_1\cdots j_i j}$ - независимые одинаково распределённые случайные величины (и их математическое ожидание одинаково), $\ev\left[ \frac{1}{b}\sum_{j = 1}^b \hat{V}_{i+1}^{j_1 \cdots j_i j} \right] = \frac{1}{b}\sum_{j = 1}^b \ev\hat{V}_{i+1}^{j_1 \cdots j_i j} = \ev\hat{V}_{i+1}^{j_1 \cdots j_i 1}$, а в силу индукционного предположения
			\begin{equation*}
			\max\left\lbrace h_i\left(X_i^{j_1\cdots j_i}\right), \ev\left[ \hat{V}_{i+1}^{j_1 \cdots j_i 1} | X_i^{j_1\cdots j_i} \right] \right\rbrace \geq \max \left\lbrace h_i\left(X_i^{j_1\cdots j_i}\right), V_i\left(X_i^{j_1\ldots j_i}\right) \right\rbrace
			\end{equation*}
			Таким образом, $\ev\left[\hat{V}_i^{j_1\ldots j_i}|X_i^{j_1\ldots j_i}\right] \geq \max \left\lbrace h_i\left(X_i^{j_1\cdots j_i}\right), V_i\left(X_i^{j_1\ldots j_i}\right) \right\rbrace$
		\end{proof}
		\par Мы также доказываем, что $\hat{V}_i^{j_1\ldots j_i}$ сходится по вероятности к $V_i\left(X_i^{j_1\ldots j_i}\right)$ при $b \to \infty$. В листьях дерева это очевидно ($\hat{V}_m^{j_1 \ldots j_m} = h_m\left(X_m^{j1 \ldots j_m}\right)$ по определению), на $i-1$ шаге цена удержания опциона $\frac{1}{b} \sum_{j = 1}^b \hat{V}_{i+1}^{j_1 \ldots j_i j}$ является средним арифметическим независимых одинаково распределённых случайных величин и сходится по закону больших чисел. Сходимость распространяется и на саму оценку в силу непрерывности операции взятия максимума. Используя тот факт, что $\forall \, a, c_1, c_2 \in \mathbb{R} \; |\max\left(a, c_1\right) - \max\left(a, c_2\right)| \leqslant |c_1 - c_2|$, мы получаем
			\begin{equation*}
			\left|\hat{V}_i^{j_1\cdots j_i} - V_i\left(X_i^{j_1 \cdots j_i}\right)\right| \leqslant \frac{1}{b}\sum_{j=1}^b\left|\hat{V}_{i+1}^{j_1 \cdots j_i j} - \ev\left[V_{i+1}\left(X_{i+1}^{j_1\cdots j_i j}\right)|X_{i+1}^{j_1\cdots j_i}\right]\right|
			\end{equation*}
			что позволяет нам вывести из сходимости на $i+1$ шаге сходимость на $i$ шаге. Подробнее в \cite{Broadie1997}
		\par Более того, асимптотически наша оценка оказывается не сдвинутой вверх, т.е. $\mathsf{E}\hat{V}_0 \to V_0\left(X_0\right)$
		\subsubsection{Оценка снизу}
		\par Значения оценки сверху в каждый момент времени -- это выбор максимума из стоимости опциона при его немедленном исполнении и математического ожидания стоимости удержания опциона. Но стоимость удержания опциона рассчитывается, исходя из дочерних узлов дерева состояний актива,то есть оценка сверху рассчитывается, опираясь на информацию о будущем. Чтобы убрать ошибку, связанную с этим, нам необходимо отделить механизм принятия решения о исполнении/удержании опциона от значений, полученных после принятия решения об удержании опциона.
		\par По сути, нам нужно оценить $\max\left\lbrace a, \ev Y \right\rbrace$ с помощью $b$ независимых одинаково распределённых реализаций случайной величины $Y$ для некоторой константы $a$ и случайной величины $Y$. Оценка $\max\left\lbrace a, \bar{Y}\right\rbrace$ (где $\bar{Y}$ -- среднее значение выборки) является оценкой сверху, так как $\ev\max\left\lbrace a, \bar{Y}\right\rbrace \geq \max\left\lbrace a, \ev\bar{Y}\right\rbrace = \max\left\lbrace a, \ev Y\right\rbrace$, что мы и использовали в построеии нашей оценки сверху.
		\par Разделим наше множество реализаций $\left\lbrace Y_i \right\rbrace _{i=1}^b$ случайной величины $Y$ на два независимых подмножества и вычислим их средние значения $\bar{Y}_1$ и $\bar{Y}_2$. Если положить
			\begin{equation}
			\hat{v} = \left\lbrace
				\begin{array}{l l}
					a, & \, \text{если } \bar{Y}_1 \leqslant a \\
					\bar{Y}_2, & \, \text{иначе} 
				\end{array}\right.
			\end{equation}
			мы отделим процесс принятия решения о исполнении/удержании опциона от оценки его стоимости (за решение будет отвечать $\bar{Y}_1$, за оценку - $\bar{Y}_2$). При этом оценка $\hat{v}$ является оценкой снизу:
			\begin{equation}
				\ev\hat{v} = \mathsf{P}\left(\bar{Y}_1 \leqslant a\right)a + \left( 1 - \mathsf{P}\left(\bar{Y}_1 \leqslant a\right) \right)\ev Y \leqslant \max\left\lbrace a, \ev Y \right\rbrace
			\end{equation}
			
		\paragraph{Нижняя оценка по \cite{Glasserman2004}} Пусть <<отвечающим за принятие решения>> подмножеством будут все реализации, кроме одной, а <<оценивающее>> множество будет состоять из одной оставшейся реализации. Возьмём математическое ожидание этой величины, т.е. положим в листьях дерева значение оценки
		    \begin{equation}
				\hat{v}_m^{j_1 j_2 \cdots j_m} = h\left( X_m^{j_1 j_2 \cdots j_m}\right)
			\end{equation}
			а для промежуточных узлов определим
		    \begin{equation}
		        \hat{v}_{ik}^{j_1 j_2 \cdots j_i} = \left\lbrace
				    \begin{array}{l l}
					    h\left( X_i^{j_1 j_2 \cdots j_i}\right), & \, \text{если } \frac{1}{b-1}\sum_{j=1, j\not= k}^b \hat{v}_{i+1}^{j_1 j_2 \cdots j_i j} \leq h\left(X_i^{j_1 j_2 \cdots j_i}\right) \\
					    \hat{v}_{i+1}^{j_1 j_2 \cdots j_i k}, & \, \text{иначе} 
				    \end{array}\right.
		    \end{equation}
		    и оценку положим равной 
		    $$
		        \hat{v}_i^{j_1 j_2 \cdots j_i} = \frac{1}{b}\sum_{k=1}^b \hat{v}_{ik}^{j_1 j_2 \cdots j_i}
		    $$


\section{Устранение экспоненциальной сложности}
	\par Начиная с некоторого момента $t_k$, когда общее число состояний достигнет некоторого $n$, мы перестанем генерировать дочерние вершины ко всем состояниям. В следующий момент времени, $t_{k+1}$, мы будем иметь всё так же $n$ состояний, а не $bn$. Этого можно достичь, если генерировать дочерние состояния не ко всем вершинам, а только к некоторым. К каким?
	\subsection{Анализ распределения состояний с помощью гистограммы}
		\par В том случае, когда состояние актива $S$ является числом в $\R ^1$, в качестве параметра $X$, распределение которого нас интересует, можно использовать  само $S$, иначе можно использовать $h(S)$. 
		\par Деля интервал $\left[\min_{i\in 1:n} X_i ; \max_{i\in 1:n} X_i + \frac{1}{n}\right)$ на $k$ равных частей $\left[a_{k-1},a_k\right)$, где $a_0 = \min_{i\in 1:n} X_i$, $a_k = \max_{i\in 1:n} X_i$, мы можем определить частоты \[f_k = \frac{1}{n}\#\left\lbrace X_i \middle\vert X_i\in\left[a_{k-1},a_k\right)\right\rbrace\] попадания событий в различные части отрезка. Из состояний, сгруппированных на отрезке $\left[a_{k-1},a_k\right)$, мы также можем создать некоторый <<средний арифметический>> вектор, кооринаты которого будут являться средним арифметическим координат всех состояний, оказавшихся на данном отрезке, и уже для этого нового среднего состояния -- представителя отрезка -- генерировать дочерние вершины в количестве $n\cdot f_k$. Для всех состояний, оказавшихся в этом отрезке, дочерними вершинами будут являться все вершины, полученные от их представителя. 
		\par Таким образом, количество рассматриваемых состояний не увеличится. С другой стороны, этот метод предполагает хранение в памяти всего дерева, а не только непосредственно обсчитываемой ветки, как это предполагалось в исходной работе \cite{Broadie1997}.
		\par Гистограммный анализ <<среза>> распределения проходит за $O\left(n\right)$ времени и за $O\left(n\right)$ по памяти, что приводит нас к общему времени работы $O\left(mn\right)$ и общей затраченной памяти $O\left(mn\right)$, где $m$ -- число дат погашения опциона, включая первую и последнюю.
	\subsection{Кластеризация состояний}
	\par Для выделения родителей будущего поколения событий можно использовать не гистограммный подход, а кластеризацию существующего поколения. Так как состояния являются векторами в $\R^d$, в качестве метрики можно взять, например, <<улучшенную>> евклидову метрику в $\R^d$:
	\[\mu\left(S_i,S_j\right) = \sqrt{\sum_{k=1}^d\frac{\left(s^i_k-s^j_k\right)}{c_k}} \text{,} \] 
	где $c_k = \left\vert\max_{i\in 1:n} s^i_k - \min_{i\in 1:n} s^i_k\right\vert$, т.е. масштабирующий множитель, уравнивающий влияние различных компонент состояний на итоговое расстояние между ними.
	\par Кластеризация может быть проведена по любому из известных алгоритмов, приведём один из наиболее простых и популярных алгоритмов кластеризации применительно к нашей задаче -- алгоритм $k$-средних.
	\begin{algorithm}
		% \SetAlgoLined
		\DontPrintSemicolon
		\SetKwFunction{Get}{Get}
		\SetKwFunction{Argmin}{argmin}
		\tcp{назначаем центроидами кластеров случайно выбранные $S_k$}
		\For{$j \in 1:k$}{
			\Get{$\alpha$}\;
			$C_j = S_{\left\lceil n\alpha\right\rceil}, \forall i \in 1:j-1 C_i \not = C_j$\;
		}
		\For{$j \in 1:n$}{
			\tcp{центроидом для каждого состояния полагаем тот из центроидов, который ближе всего к данному состоянию}
			$S_j$.centroid = \Argmin{$i\in 1:k$, $\mu\left(S_j, C_i\right)$}\;
		}
		changed = true\;
		\Repeat{changed = true}{
			\For{$j \in 1:k$}{
				centroid = $\left\lbrace i\in 1: n\middle\vert S_i\text{.centroid} = C_j\right\rbrace$\;
				$C_j = \frac{1}{\#\left(\text{centroid}\right)}\sum\limits_{i \in \text{centroid}}S_i$\;
			}
			changed = false\;
			\For{$j \in 1:n$}{
				\tcp{пересчитываем принадлежность состояний центроидам}
				oldcentroid = $S_j$.centroid\;
				$S_j$.centroid = \Argmin{$i\in 1:k$, $\mu\left(S_j, C_i\right)$}\;
				changed = (oldcentroid == $S_j$.centroid)
			}
		}
	\end{algorithm}
	\par В этом случае набор центроидов $\left\lbrace C_j\right\rbrace_{j=1}^k$ можно использовать в качестве родителей для следующего поколения состояний.
	\par Известные недостатки алгоритма $k$-средних, такие как сложность $O\left(2^n\right)$ в худшем случае, зависимость результатов от начального выбора центроидов (которая, впрочем, может быть частично устранена при модификации алгоритма, например, в версии $k$-means++, описанной в \cite{Arthur2007}), влияют на итоговую сложность алгоритма. Как и в случае гистограммного анализа, здесь потребуется $O\left(mn\right)$ памяти в силу необходимость хранить все когда-либо сгенерированные узлы, но затраты на анализ среза будут равны $O\left(n\right)$ в лучшем случае и $O\left(2^n\right)$ в худшем, что ведёт к сложности $O\left(m\cdot 2^n\right)$ для всего процесса оценки в худшем случае.
	\par Возможным преимуществом перед гистограммным анализом может оказаться более <<честный>> выбор представителя группы, не зависящий от того, как именно группа состояний распределена относительно границ отрезков, на которые мы делим область распеделения.
	\section{Выбор $k$}
	\par Выбор количества родителей для будущего поколения -- отдельный вопрос. Скорее всего, количество родителей будет непосредственно влиять на величину доверительного интервала результата.
\nocite{*}
\printbibliography
\end{document}